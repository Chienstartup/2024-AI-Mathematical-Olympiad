{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPCJjsa4piGvRf3BSZ8O/II",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b717c4463e74eca8b3ed5c96b11cb9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65952835e4db412295152d175c6d28aa",
              "IPY_MODEL_3c5c115d573b451fa5b9f8d94e41410a",
              "IPY_MODEL_31c3cc6fe9c146518a591e388baea7f1"
            ],
            "layout": "IPY_MODEL_685dc6aa902941f9bf9362a372bb35d2"
          }
        },
        "65952835e4db412295152d175c6d28aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcd9623cfbe5477e980975167337641e",
            "placeholder": "​",
            "style": "IPY_MODEL_fa30b29ed57b425e86eb0dee254ae868",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3c5c115d573b451fa5b9f8d94e41410a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad439ddd6f5240b78963b869eb61965c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_166d8deb30994a06937c29b79483c526",
            "value": 2
          }
        },
        "31c3cc6fe9c146518a591e388baea7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94073a9301f94204ab476351824cb843",
            "placeholder": "​",
            "style": "IPY_MODEL_6277ba03d0ba4a8d91d2adbeef604cb1",
            "value": " 2/2 [00:09&lt;00:00,  4.60s/it]"
          }
        },
        "685dc6aa902941f9bf9362a372bb35d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcd9623cfbe5477e980975167337641e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa30b29ed57b425e86eb0dee254ae868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad439ddd6f5240b78963b869eb61965c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "166d8deb30994a06937c29b79483c526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94073a9301f94204ab476351824cb843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6277ba03d0ba4a8d91d2adbeef604cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84aaea2896d04b669e38e75154a21a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4b22dc85be04cf6a4c49d1296f4e164",
              "IPY_MODEL_8cea27499af44fe78683a7fa04127f19",
              "IPY_MODEL_17122fa8f0b84e87ac036db744e6785e"
            ],
            "layout": "IPY_MODEL_7723ecb661734ff6ae71ce08fdf422cc"
          }
        },
        "a4b22dc85be04cf6a4c49d1296f4e164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8561b2ceb73e44419d7ca5f0fa0a7dfc",
            "placeholder": "​",
            "style": "IPY_MODEL_cfbb20a9921349eb90f50147948d256a",
            "value": "Map: 100%"
          }
        },
        "8cea27499af44fe78683a7fa04127f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_690f0fd7a5d945908be99772f7243ccb",
            "max": 6755,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de1f9ad0c85940fa85ba0b0578235338",
            "value": 6755
          }
        },
        "17122fa8f0b84e87ac036db744e6785e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcd05934e01349d4902df4f8af0c1dc9",
            "placeholder": "​",
            "style": "IPY_MODEL_4fd40b3d65ad4e77871e9205aa309316",
            "value": " 6755/6755 [00:03&lt;00:00, 1862.20 examples/s]"
          }
        },
        "7723ecb661734ff6ae71ce08fdf422cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8561b2ceb73e44419d7ca5f0fa0a7dfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfbb20a9921349eb90f50147948d256a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "690f0fd7a5d945908be99772f7243ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1f9ad0c85940fa85ba0b0578235338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcd05934e01349d4902df4f8af0c1dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fd40b3d65ad4e77871e9205aa309316": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db79025b44a9439cb74bdf3f97ab5193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4be1254b71b5473f9d7c2a4bff616740",
              "IPY_MODEL_3d8605f2d13944ed9becc4d44ba1e4f1",
              "IPY_MODEL_7fe7490f56a047dcb41a714b28b65559"
            ],
            "layout": "IPY_MODEL_97ef57a373464cdaaf3a2ba7962ec3f4"
          }
        },
        "4be1254b71b5473f9d7c2a4bff616740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63d304509713407ea44e848f78b6b404",
            "placeholder": "​",
            "style": "IPY_MODEL_08274fd22ea24b0ab7b0eabea50c5817",
            "value": "Map: 100%"
          }
        },
        "3d8605f2d13944ed9becc4d44ba1e4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a65f26d0c92e45f7970a9465251f0f0f",
            "max": 1689,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcce1a28396a465db99e3281b495ee59",
            "value": 1689
          }
        },
        "7fe7490f56a047dcb41a714b28b65559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59eb33891dd14357aeba1a1f381a2459",
            "placeholder": "​",
            "style": "IPY_MODEL_42b7a45c750f40c1a36d3c842f7090c4",
            "value": " 1689/1689 [00:00&lt;00:00, 1866.93 examples/s]"
          }
        },
        "97ef57a373464cdaaf3a2ba7962ec3f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63d304509713407ea44e848f78b6b404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08274fd22ea24b0ab7b0eabea50c5817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a65f26d0c92e45f7970a9465251f0f0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcce1a28396a465db99e3281b495ee59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59eb33891dd14357aeba1a1f381a2459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42b7a45c750f40c1a36d3c842f7090c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chienstartup/2024-AI-Mathematical-Olympiad/blob/main/SOTA_model_PEFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Setting Environment"
      ],
      "metadata": {
        "id": "elTbQHSuIdJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyArrow and Datasets are often used together in machine learning pipelines to:\n",
        "\n",
        "- Load and preprocess large datasets efficiently\n",
        "- Prepare data for model training with minimal memory overhead\n",
        "- Share and version datasets easily among team members or the wider community"
      ],
      "metadata": {
        "id": "7TNONa1aG36l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#after install transformers will restart the kernal\n",
        "!pip install transformers==4.33.0\n",
        "import transformers\n",
        "print(f\"Transformers version: {transformers.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU3UwkupgZ5v",
        "outputId": "9bd2397c-5d93-4b93-f69c-cbb59bd603d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.33.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyarrow datasets -qq"
      ],
      "metadata": {
        "id": "R90AvmhKFXfe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft -qq"
      ],
      "metadata": {
        "id": "Pc9zx86dhc04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once installed, you need to restart the notebook\n",
        "pip install -U bitsandbytes -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6fIUW50K6aT",
        "outputId": "4a92ad01-f10d-4be4-b5ef-706d5e0d44ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.6.68)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -qq"
      ],
      "metadata": {
        "id": "pRPjv6EVCLrR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import Dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "kORw06fWM3DB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate user\n",
        "from google.colab import auth\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your huggingface token.\n",
        "hf_token = \"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\"\n",
        "\n",
        "# Save the token in the os environment\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "# Login\n",
        "!huggingface-cli login --token $hf_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5smLo4fJRXBr",
        "outputId": "975be802-66ea-496f-8c0e-8a48cf33a0a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, BitsAndBytesConfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5kInHdLHNxG",
        "outputId": "9e87b9ee-4a13-48c7-9b27-62a5133e6ab4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- AutoTokenizer:\n",
        "This class automatically loads the appropriate tokenizer for a given model. It converts text into tokens that the model can understand.\n",
        "- AutoModelForCausalLM:\n",
        "This loads a pre-trained causal language model for a given architecture. It's used for tasks like text generation where the model predicts the next token based on previous tokens.\n",
        "- DataCollatorForSeq2Seq:\n",
        "This prepares batches of data for sequence-to-sequence models. It handles padding and creates attention masks for efficient training.\n",
        "- TrainingArguments:\n",
        "This class defines various parameters for model training, such as learning rate, batch size, and number of epochs.\n",
        "- Trainer:\n",
        "A high-level API that simplifies the training process. It handles the training loop, evaluation, and saving of models.\n",
        "- BitsAndBytesConfig:\n",
        "This configures quantization settings for models, allowing for reduced memory usage and faster inference, often used in techniques like 4-bit or 8-bit quantization."
      ],
      "metadata": {
        "id": "Z4AXs1scF1Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Loading Train dataset"
      ],
      "metadata": {
        "id": "ZuqpLIIPBLwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('https://raw.githubusercontent.com/Chienstartup/2024-AI-Mathematical-Olympiad/main/data%20source/train.csv')\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FZ-6X1f6BLBw",
        "outputId": "b8a81499-abc1-4fc8-b5bc-f3d81d54b629"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id                                            problem  answer\n",
              "0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      52\n",
              "1  246d26  Each of the three-digits numbers $111$ to $999...     250\n",
              "2  2fc4ad  Let the `sparkle' operation on positive intege...     702\n",
              "3  430b63  What is the minimum value of $5x^2+5y^2-8xy$ w...     800\n",
              "4  5277ed  There exists a unique increasing geometric seq...     211"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ede1f158-8a40-4018-8716-cc47b674103e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>problem</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>229ee8</td>\n",
              "      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>246d26</td>\n",
              "      <td>Each of the three-digits numbers $111$ to $999...</td>\n",
              "      <td>250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2fc4ad</td>\n",
              "      <td>Let the `sparkle' operation on positive intege...</td>\n",
              "      <td>702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>430b63</td>\n",
              "      <td>What is the minimum value of $5x^2+5y^2-8xy$ w...</td>\n",
              "      <td>800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5277ed</td>\n",
              "      <td>There exists a unique increasing geometric seq...</td>\n",
              "      <td>211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ede1f158-8a40-4018-8716-cc47b674103e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ede1f158-8a40-4018-8716-cc47b674103e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ede1f158-8a40-4018-8716-cc47b674103e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9b3cb91a-83df-475c-9818-7c71b49b6bf0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b3cb91a-83df-475c-9818-7c71b49b6bf0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9b3cb91a-83df-475c-9818-7c71b49b6bf0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"bedda4\",\n          \"246d26\",\n          \"739bc9\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"problem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Let $ABCD$ be a unit square. Let $P$ be the point on $AB$ such that $|AP| = 1/{20}$ and let $Q$ be the point on $AD$ such that $|AQ| = 1/{24}$. The lines $DP$ and $BQ$ divide the square into four regions. Find the ratio between the areas of the largest region and the smallest region.\",\n          \"Each of the three-digits numbers $111$ to $999$ is coloured blue or yellow in such a way that the sum of any two (not necessarily different) yellow numbers is equal to a blue number. What is the maximum possible number of yellow numbers there can be?\",\n          \"For how many positive integers $m$ does the equation \\\\[\\\\vert \\\\vert x-1 \\\\vert -2 \\\\vert=\\\\frac{m}{100}\\\\] have $4$ distinct solutions?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 243,\n        \"min\": 52,\n        \"max\": 800,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          320,\n          250,\n          199\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Quantization: to reduce model memory usage and accelerate inference"
      ],
      "metadata": {
        "id": "UbXDd379IqGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Precision Setting:\n",
        "\n",
        "  load_in_4bit = True: Uses 4-bit precision, significantly reducing memory usage.</br>\n",
        "  Other options: 8-bit precision (load_in_8bit = True) or no quantization (default).</br>\n",
        "  4-bit vs 8-bit: 4-bit offers greater memory savings but may slightly reduce accuracy; 8-bit balances memory savings and precision.\n",
        "\n",
        "\n",
        "- Quantization Type:\n",
        "\n",
        "  bnb_4bit_quant_type=\"nf4\": Uses Normal Float 4, optimized for normally distributed weights..</br>\n",
        "  Other options: Such as \"fp4\" (Float Point 4)..</br>\n",
        "  NF4 vs FP4: NF4 is suitable for normally distributed weights, FP4 provides a wider dynamic range.\n",
        "\n",
        "\n",
        "- Computation Data Type:\n",
        "\n",
        "  bnb_4bit_compute_dtype=torch.bfloat16: Uses bfloat16 for computations.\n",
        "  bfloat16 vs float16:\n",
        "\n",
        "  bfloat16 has a larger exponent range, suitable for large values and gradients in deep learning..</br>\n",
        "  float16 provides higher precision for small values..</br>\n",
        "  bfloat16 typically performs better in terms of training stability and hardware compatibility.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Double Quantization:\n",
        "\n",
        "  bnb_4bit_use_double_quant=True: Enables double quantization, further compressing the model..</br>\n",
        "  Without double quantization, one can use load_in_4bit or load_in_8bit alone..</br>\n",
        "  Double quantization provides additional memory savings but may slightly increase computational overhead."
      ],
      "metadata": {
        "id": "scTrX5BUI3tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit = True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )"
      ],
      "metadata": {
        "id": "DduTPcYQBlv6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading model from huggingface might take times and waste your computing credits, please check whether you have enough computing resource"
      ],
      "metadata": {
        "id": "d3-uFP5RMQhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/deepseek-math-7b-rl\",\n",
        "    device_map = \"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "0b717c4463e74eca8b3ed5c96b11cb9b",
            "65952835e4db412295152d175c6d28aa",
            "3c5c115d573b451fa5b9f8d94e41410a",
            "31c3cc6fe9c146518a591e388baea7f1",
            "685dc6aa902941f9bf9362a372bb35d2",
            "fcd9623cfbe5477e980975167337641e",
            "fa30b29ed57b425e86eb0dee254ae868",
            "ad439ddd6f5240b78963b869eb61965c",
            "166d8deb30994a06937c29b79483c526",
            "94073a9301f94204ab476351824cb843",
            "6277ba03d0ba4a8d91d2adbeef604cb1"
          ]
        },
        "id": "dd_ne_TSBLF1",
        "outputId": "f434ed5b-ba25-4dc7-dde5-231bce057671"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b717c4463e74eca8b3ed5c96b11cb9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The pipeline is a high-level abstraction that simplifies the process of using pre-trained models for various natural language processing (NLP) tasks.\n",
        "The main pipeline types include:\n",
        "\n",
        "- Text Classification (text-classification): Classifies text, such as sentiment analysis.\n",
        "- Text Generation (text-generation): Generates new text, like stories or dialogues.\n",
        "- Named Entity Recognition (ner): Identifies entities in text, such as names, places, organizations.\n",
        "- Question Answering (question-answering): Answers questions based on given context.\n",
        "- Summarization (summarization): Generates summaries of text.\n",
        "- Translation (translation): Translates text from one language to another.\n",
        "- Feature Extraction (feature-extraction): Extracts feature vectors from text.\n",
        "- Fill-Mask (fill-mask): Fills in masked tokens in sentences.\n",
        "- Zero-Shot Classification (zero-shot-classification): Performs classification without specific training data.\n",
        "- Text-to-Text Generation (text2text-generation): Various text-to-text tasks like translation or summarization.\n",
        "- Conversational (conversational): Engages in conversational interactions.\n",
        "- Image Classification (image-classification): Classifies images.\n",
        "- Audio Classification (audio-classification): Classifies audio.\n",
        "- Automatic Speech Recognition (automatic-speech-recognition): Converts speech to text.\n",
        "- Visual Question Answering (visual-question-answering): Answers questions based on images.\n",
        "\n",
        "Using these pipelines allows for quick implementation of complex NLP tasks without needing to delve into the details of the underlying models."
      ],
      "metadata": {
        "id": "Li_uJnxGNGmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "7Rk3ne2RgXKv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-math-7b-rl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGrWsWmVFxcf",
        "outputId": "5b7e2eff-5c0e-4734-e96f-e0b046aaf5e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Testing Pretrained Deepseek model"
      ],
      "metadata": {
        "id": "aBYAXpEfTQJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype='auto',\n",
        "    device_map= 'auto',\n",
        "    max_new_tokens = 4500\n",
        ")"
      ],
      "metadata": {
        "id": "nAkP7rsQBLNF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem1 = train_df.problem[0]"
      ],
      "metadata": {
        "id": "WcY6i2gHBLP9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem1"
      ],
      "metadata": {
        "id": "6kjPyRxRIKzd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "83b82953-5c4b-44ba-9859-ee2a6e3a21fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Let $k, l > 0$ be parameters. The parabola $y = kx^2 - 2kx + l$ intersects the line $y = 4$ at two points $A$ and $B$. These points are distance 6 apart. What is the sum of the squares of the distances from $A$ and $B$ to the origin?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Ensemble Prompt to Problem Set"
      ],
      "metadata": {
        "id": "xXY2RPPgILeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(question):\n",
        "\n",
        "    description = \"\"\"Below is a math problem you are to solve (positive numerical answer):\n",
        "    \\\"{}\\\"\n",
        "    To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step.\n",
        "    Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\n",
        "    Write the entire script covering all the steps (use comments and document it well) and print the result.\n",
        "    After solving the problem, output the final numerical answer within \\\\boxed{}. You should not repeat the problem or instruction in your answer.\n",
        "    Approach:\"\"\"\n",
        "\n",
        "    prompt = f\"\"\" {description}. Here is the question: {question}\"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "SZiQFKN4IB5C"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_problem = generate_prompt(problem1)"
      ],
      "metadata": {
        "id": "biF7HnxDIQ-i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_problem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "WMmZMFoYIVbB",
        "outputId": "3ddcf690-58c1-44a2-fb82-259c5d16a0cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Below is a math problem you are to solve (positive numerical answer):\\n    \"{}\"\\n    To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. \\n    Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\\n    Write the entire script covering all the steps (use comments and document it well) and print the result. \\n    After solving the problem, output the final numerical answer within \\\\boxed{}. You should not repeat the problem or instruction in your answer.\\n    Approach:. Here is the question: Let $k, l > 0$ be parameters. The parabola $y = kx^2 - 2kx + l$ intersects the line $y = 4$ at two points $A$ and $B$. These points are distance 6 apart. What is the sum of the squares of the distances from $A$ and $B$ to the origin?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(prompt_problem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkqFU23MBLTF",
        "outputId": "f38651ee-297d-488a-a03e-26920ba75fc5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:100001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tJ8EShSHxYx",
        "outputId": "6651f9c0-0bb4-43c6-d5b4-b7fa3f4ef3fd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'generated_text': ' Below is a math problem you are to solve (positive numerical answer):\\n    \"{}\"\\n    To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. \\n    Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!\\n    Write the entire script covering all the steps (use comments and document it well) and print the result. \\n    After solving the problem, output the final numerical answer within \\\\boxed{}. You should not repeat the problem or instruction in your answer.\\n    Approach:. Here is the question: Let $k, l > 0$ be parameters. The parabola $y = kx^2 - 2kx + l$ intersects the line $y = 4$ at two points $A$ and $B$. These points are distance 6 apart. What is the sum of the squares of the distances from $A$ and $B$ to the origin?\\nTo solve this problem, we first need to find the values of $k$ and $l$ that make the parabola $y = kx^2 - 2kx + l$ intersect the line $y = 4$ at two points $A$ and $B$ that are distance 6 apart.\\n\\nLet\\'s set $y = 4$ in the equation of the parabola:\\n\\\\[4 = kx^2 - 2kx + l.\\\\]\\nRearranging terms, we get:\\n\\\\[kx^2 - 2kx + (l - 4) = 0.\\\\]\\nThis is a quadratic equation in $x$. Let\\'s call its roots $x_1$ and $x_2$, which represent the $x$-coordinates of points $A$ and $B$ respectively. Since the distance between $A$ and $B$ is 6, we have:\\n\\\\[(x_2 - x_1)^2 = 6^2 = 36.\\\\]\\nBy Vieta\\'s formulas, we know that $x_1 + x_2 = \\\\frac{2k}{k} = 2$ and $x_1x_2 = \\\\frac{l - 4}{k}$. We can use these to find an expression for $(x_2 - x_1)^2$:\\n\\\\[(x_2 - x_1)^2 = (x_2 + x_1)^2 - 4x_1x_2 = 4 - 4\\\\frac{l - 4}{k}.\\\\]\\nSubstituting the given value of $(x_2 - x_1)^2$, we get:\\n\\\\[36 = 4 - 4\\\\frac{l - 4}{k}.\\\\]\\nSimplifying, we have:\\n\\\\[4\\\\frac{l - 4}{k} = -32.\\\\]\\nDividing both sides by 4, we get:\\n\\\\[\\\\frac{l - 4}{k} = -8.\\\\]\\nMultiplying both sides by $k$, we have:\\n\\\\[l - 4 = -8k.\\\\]\\nRearranging terms, we get:\\n\\\\[l = -8k + 4.\\\\]\\nNow, we need to find the sum of the squares of the distances from $A$ and $B$ to the origin. The distance from a point $(x, y)$ to the origin is $\\\\sqrt{x^2 + y^2}$. For point $A$ with coordinates $(x_1, 4)$, the distance is $\\\\sqrt{x_1^2 + 4^2} = \\\\sqrt{x_1^2 + 16}$. Similarly, for point $B$ with coordinates $(x_2, 4)$, the distance is $\\\\sqrt{x_2^2 + 16}$. The sum of the squares of these distances is:\\n\\\\[(\\\\sqrt{x_1^2 + 16})^2 + (\\\\sqrt{x_2^2 + 16})^2 = x_1^2 + 16 + x_2^2 + 16 = x_1^2 + x_2^2 + 32.\\\\]\\nBy Vieta\\'s formulas, we know that $x_1^2 + x_2^2 = (x_1 + x_2)^2 - 2x_1x_2 = 2^2 - 2\\\\frac{l - 4}{k} = 4 + 16 = 20$. Therefore, the sum of the squares of the distances from $A$ and $B$ to the origin is:\\n\\\\[20 + 32 = 52.\\\\]\\nSo, the sum of the squares of the distances from $A$ and $B$ to the origin is 52. The answer is: $52$'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Loading GMS8K dataset:\n",
        "<h3> from https://www.kaggle.com/datasets/abdullahmeda/annotated-math-and-gsm8k-datasets"
      ],
      "metadata": {
        "id": "nu95jY-EPLjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file using Pandas\n",
        "csv_file_path = \"https://raw.githubusercontent.com/Chienstartup/2024-AI-Mathematical-Olympiad/main/data%20source/gsm8k-annotated-non-gpt4.csv\"\n",
        "df = pd.read_csv(csv_file_path)"
      ],
      "metadata": {
        "id": "ufCKvcHFOs26"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "1pfvX9W1hL6B",
        "outputId": "5c61b7fa-97ea-4486-de25-dd1593cec360"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               question  \\\n",
              "0     Natalia sold clips to 48 of her friends in Apr...   \n",
              "1     Weng earns $12 an hour for babysitting. Yester...   \n",
              "2     Betty is saving money for a new wallet which c...   \n",
              "3     Julie is reading a 120-page book. Yesterday, s...   \n",
              "4     James writes a 3-page letter to 2 different fr...   \n",
              "...                                                 ...   \n",
              "8439  John had a son James when he was 19.  James is...   \n",
              "8440  There are some oranges in a basket. Ana spends...   \n",
              "8441  Mark's car breaks down and he needs to get a n...   \n",
              "8442  Farmer Brown has 20 animals on his farm, all e...   \n",
              "8443  Henry and 3 of his friends order 7 pizzas for ...   \n",
              "\n",
              "                                                 answer  \\\n",
              "0     Natalia sold 48/2 = <<48/2=24>>24 clips in May...   \n",
              "1     Weng earns 12/60 = $<<12/60=0.2>>0.2 per minut...   \n",
              "2     In the beginning, Betty has only 100 / 2 = $<<...   \n",
              "3     Maila read 12 x 2 = <<12*2=24>>24 pages today....   \n",
              "4     He writes each friend 3*2=<<3*2=6>>6 pages a w...   \n",
              "...                                                 ...   \n",
              "8439  Dora is 12-3=<<12-3=9>>9\\nSo James is 9*2=<<9*...   \n",
              "8440  There are 60 minutes in an hour. Ana peels an ...   \n",
              "8441  The discount on the radiator was 400*.8=$<<400...   \n",
              "8442  Let C be the number of chickens.\\nThere are 20...   \n",
              "8443  There are 7*8=<<7*8=56>>56 slices in total.\\nT...   \n",
              "\n",
              "                                          code_solution  boxed_number  \n",
              "0     ```python\\n# Step 1: Define the number of clip...            72  \n",
              "1     ```python\\n# Step 1: Define the hourly wage an...            10  \n",
              "2     ```python\\n# Step 1: Calculate the initial amo...             5  \n",
              "3     ```python\\n# Step 1: Define the number of page...            42  \n",
              "4     ```python\\n# Import the necessary libraries (n...           624  \n",
              "...                                                 ...           ...  \n",
              "8439  ```python\\n# Step 1: Calculate Dora's current ...             8  \n",
              "8440  ```python\\n# Calculate the number of oranges A...             5  \n",
              "8441  ```python\\n# Step 1: Calculate the discount on...           230  \n",
              "8442  ```python\\n# Import the necessary libraries\\ni...             5  \n",
              "8443  ```python\\n# Step 1: Calculate the total numbe...            14  \n",
              "\n",
              "[8444 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85131b7f-3156-43ad-baad-bf5c44e158e0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>code_solution</th>\n",
              "      <th>boxed_number</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Natalia sold clips to 48 of her friends in Apr...</td>\n",
              "      <td>Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May...</td>\n",
              "      <td>```python\\n# Step 1: Define the number of clip...</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Weng earns $12 an hour for babysitting. Yester...</td>\n",
              "      <td>Weng earns 12/60 = $&lt;&lt;12/60=0.2&gt;&gt;0.2 per minut...</td>\n",
              "      <td>```python\\n# Step 1: Define the hourly wage an...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Betty is saving money for a new wallet which c...</td>\n",
              "      <td>In the beginning, Betty has only 100 / 2 = $&lt;&lt;...</td>\n",
              "      <td>```python\\n# Step 1: Calculate the initial amo...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Julie is reading a 120-page book. Yesterday, s...</td>\n",
              "      <td>Maila read 12 x 2 = &lt;&lt;12*2=24&gt;&gt;24 pages today....</td>\n",
              "      <td>```python\\n# Step 1: Define the number of page...</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>James writes a 3-page letter to 2 different fr...</td>\n",
              "      <td>He writes each friend 3*2=&lt;&lt;3*2=6&gt;&gt;6 pages a w...</td>\n",
              "      <td>```python\\n# Import the necessary libraries (n...</td>\n",
              "      <td>624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8439</th>\n",
              "      <td>John had a son James when he was 19.  James is...</td>\n",
              "      <td>Dora is 12-3=&lt;&lt;12-3=9&gt;&gt;9\\nSo James is 9*2=&lt;&lt;9*...</td>\n",
              "      <td>```python\\n# Step 1: Calculate Dora's current ...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8440</th>\n",
              "      <td>There are some oranges in a basket. Ana spends...</td>\n",
              "      <td>There are 60 minutes in an hour. Ana peels an ...</td>\n",
              "      <td>```python\\n# Calculate the number of oranges A...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8441</th>\n",
              "      <td>Mark's car breaks down and he needs to get a n...</td>\n",
              "      <td>The discount on the radiator was 400*.8=$&lt;&lt;400...</td>\n",
              "      <td>```python\\n# Step 1: Calculate the discount on...</td>\n",
              "      <td>230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8442</th>\n",
              "      <td>Farmer Brown has 20 animals on his farm, all e...</td>\n",
              "      <td>Let C be the number of chickens.\\nThere are 20...</td>\n",
              "      <td>```python\\n# Import the necessary libraries\\ni...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8443</th>\n",
              "      <td>Henry and 3 of his friends order 7 pizzas for ...</td>\n",
              "      <td>There are 7*8=&lt;&lt;7*8=56&gt;&gt;56 slices in total.\\nT...</td>\n",
              "      <td>```python\\n# Step 1: Calculate the total numbe...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8444 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85131b7f-3156-43ad-baad-bf5c44e158e0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-85131b7f-3156-43ad-baad-bf5c44e158e0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-85131b7f-3156-43ad-baad-bf5c44e158e0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2ae5f1c5-1fb0-4f24-827b-9672a7179f82\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2ae5f1c5-1fb0-4f24-827b-9672a7179f82')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2ae5f1c5-1fb0-4f24-827b-9672a7179f82 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b1a73060-e689-44f1-b5ea-be0fb70ec7bb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b1a73060-e689-44f1-b5ea-be0fb70ec7bb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8444,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8444,\n        \"samples\": [\n          \"One pie costs $4 for a piece. Each pie is having 3 pieces. During one hour the bakery can make 12 pies. Creating one pie costs the bakery $0.5. Considering the bakery would be able to sell all pie pieces, how much money would it make?\",\n          \"Maximoff's monthly bill is $60 per month. His monthly bill increased by thirty percent when he started working at home. How much is his total monthly bill working from home?\",\n          \"The teacher decided to rewards his students with extra recess on report card day if they got good grades. Students normally get 20 minutes for recess. He told the students that every A got them 2 extra minutes of recess. Every B got them one extra minute. Every C got them zero extra minutes, but every D got them 1 less minute. When report cards came out there were 10 As, 12 Bs, 14Cs, and 5Ds. In total, how much recess would the students get that day?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8444,\n        \"samples\": [\n          \"If the bakery can make 12 pies, this means there would be 12 * 3 = <<12*3=36>>36 pie pieces.\\nFor all the pieces the bakery would make 36 * 4 = $<<36*4=144>>144.\\nThe cost of making 12 pies is 12 * 0.5 = $<<12*0.5=6>>6.\\nThat means the bakery would make 144 - 6 = $<<144-6=138>>138.\\n#### 138\",\n          \"He pays an additional $60 x 0.30= $<<60*0.3=18>>18 on his monthly bill working from home.\\nTherefore, the total amount he pays for his monthly bill working from home is $60 + $18= $<<60+18=78>>78.\\n#### 78\",\n          \"The students have 20 minutes to start.\\nThey get 20 minutes added for the As because 10 times 2 equals <<10*2=20>>20\\nThey get 12 minutes added for the Bs because 12 times 1 equals <<12*1=12>>12\\nThey get no minutes added or subtracted for the Cs because 14 times 0 equals 0.\\nThey get 5 minutes taken away for the Ds because 5 times 1 equals <<5*1=5>>5.\\nIn total, they get 47 minutes of recess because 20 plus 20 plus 12 minus 5 equals <<20+20+12-5=47>>47.\\n#### 47\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"code_solution\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8444,\n        \"samples\": [\n          \"```python\\n# Step 1: Calculate the total number of pie pieces\\npies_made = 12  # number of pies made in an hour\\npieces_per_pie = 3\\ntotal_pie_pieces = pies_made * pieces_per_pie\\n\\n# Step 2: Calculate the revenue from selling all pie pieces\\nprice_per_piece = 4\\nrevenue = total_pie_pieces * price_per_piece\\n\\n# Step 3: Calculate the cost of making 12 pies\\ncost_per_pie = 0.5\\ncost = pies_made * cost_per_pie\\n\\n# Step 4: Calculate the profit\\nprofit = revenue - cost\\n\\n# Print the final answer\\nprint(int(profit))  # output: 138\\n```\",\n          \"```python\\n# Step 1: Define the monthly bill\\nmonthly_bill = 60\\n\\n# Step 2: Calculate the increase in monthly bill (30% of $60)\\nincrease = monthly_bill * 0.30\\n\\n# Step 3: Calculate the total monthly bill working from home\\ntotal_bill = monthly_bill + increase\\n\\n# Print the total monthly bill\\nprint(int(total_bill))  # Output: 78\\n```\",\n          \"```python\\n# Step 1: Initialize the base recess time\\nbase_recess_time = 20\\n\\n# Step 2: Calculate the extra time for As\\nas_extra_time = 10 * 2\\n\\n# Step 3: Calculate the extra time for Bs\\nbs_extra_time = 12 * 1\\n\\n# Step 4: Calculate the subtracted time for Ds\\nds_subtracted_time = 5 * 1\\n\\n# Step 5: Calculate the total recess time\\ntotal_recess_time = base_recess_time + as_extra_time + bs_extra_time - ds_subtracted_time\\n\\n# Print the final answer\\nprint(total_recess_time)\\n```\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"boxed_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2110507,\n        \"min\": 0,\n        \"max\": 192000000,\n        \"num_unique_values\": 883,\n        \"samples\": [\n          840,\n          70000,\n          9400\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the DataFrame to a Hugging Face dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Display some information about the dataset\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HidMlHd_hH2M",
        "outputId": "e0fa72d1-e64f-47fd-ad58-0c3d7e6c3551"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['question', 'answer', 'code_solution', 'boxed_number'],\n",
            "    num_rows: 8444\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert a pandas DataFrame into a Hugging Face Dataset object."
      ],
      "metadata": {
        "id": "lARN5gTtV0kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gms8k_dataset = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "7hc4K0qsCRaT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict"
      ],
      "metadata": {
        "id": "gWTypbuSB0fe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training (80%) and validation (20%) sets\n",
        "split_dataset = gms8k_dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Create a DatasetDict to hold the train and validation datasets\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': split_dataset['train'],\n",
        "    'validation': split_dataset['test']\n",
        "})"
      ],
      "metadata": {
        "id": "Cs5QEknBO0h-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRqsaw14B4sc",
        "outputId": "f744d8a9-f1c7-43e5-b7b0-5208818d5a92"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'answer', 'code_solution', 'boxed_number'],\n",
              "        num_rows: 6755\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'answer', 'code_solution', 'boxed_number'],\n",
              "        num_rows: 1689\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Process Data in to transformer Datadict"
      ],
      "metadata": {
        "id": "v9UAkf3APRv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- tokenizer.pad_token = tokenizer.eos_token:\n",
        "\n",
        "  This line sets the padding token to be the same as the end-of-sequence token.\n",
        "  This is typically used to ensure the model consistently identifies the end of sequences when dealing with varying lengths.\n",
        "\n",
        "- max_length = 600:\n",
        "\n",
        "  Sets the maximum length for both inputs and labels to 600 tokens.\n",
        "\n",
        "\n",
        "- model_inputs = tokenizer(example[\"question\"], max_length=max_length, truncation=True, padding='max_length'):\n",
        "\n",
        "  Processes the \"question\" field using the tokenizer.</br>\n",
        "  max_length=max_length: Limits the maximum length.</br>\n",
        "  truncation=True: Truncates text if it exceeds the maximum length.</br>\n",
        "  padding='max_length': Pads all sequences to the maximum length.\n",
        "\n",
        "\n",
        "- labels = tokenizer(example[\"code_solution\"], max_length=max_length, truncation=True, padding='max_length'):\n",
        "\n",
        "  Similarly processes the \"code_solution\" field as labels.</br>\n",
        "\n",
        "- model_inputs[\"labels\"] = labels[\"input_ids\"]:\n",
        "\n",
        "  Adds the processed label input IDs to the model inputs.</br>\n",
        "\n",
        "- return model_inputs:\n",
        "\n",
        "  Returns the processed model inputs, including input IDs and corresponding labels.\n",
        "\n",
        "\n",
        "\n",
        "The purpose of this function is to convert raw text data into a format directly usable by the model, including converting text to token IDs and ensuring all inputs and labels have consistent lengths."
      ],
      "metadata": {
        "id": "Xz92cGJWXb8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the padding token to be the same as the eos token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def preprocess_function(example, tokenizer=tokenizer):\n",
        "    max_length = 600  # Set a consistent max_length for both input and labels\n",
        "    model_inputs = tokenizer(example[\"question\"], max_length=max_length, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(example[\"code_solution\"], max_length=max_length,truncation=True, padding='max_length')\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "Ono9liJ9PJFt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The main purpose of this code is to:\n",
        "\n",
        "  Apply the preprocessing function to each sample in the dataset.</br>\n",
        "  Perform this in batch mode for efficiency.</br>\n",
        "  Remove the original columns, keeping only the preprocessed data.</br>\n",
        "  Create a new dataset containing the preprocessed data.</br>"
      ],
      "metadata": {
        "id": "yGc_GIDYYYqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_datasets = dataset_dict.map(preprocess_function,  batched = True, remove_columns = dataset_dict[\"train\"].column_names )\n",
        "processed_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "84aaea2896d04b669e38e75154a21a43",
            "a4b22dc85be04cf6a4c49d1296f4e164",
            "8cea27499af44fe78683a7fa04127f19",
            "17122fa8f0b84e87ac036db744e6785e",
            "7723ecb661734ff6ae71ce08fdf422cc",
            "8561b2ceb73e44419d7ca5f0fa0a7dfc",
            "cfbb20a9921349eb90f50147948d256a",
            "690f0fd7a5d945908be99772f7243ccb",
            "de1f9ad0c85940fa85ba0b0578235338",
            "bcd05934e01349d4902df4f8af0c1dc9",
            "4fd40b3d65ad4e77871e9205aa309316",
            "db79025b44a9439cb74bdf3f97ab5193",
            "4be1254b71b5473f9d7c2a4bff616740",
            "3d8605f2d13944ed9becc4d44ba1e4f1",
            "7fe7490f56a047dcb41a714b28b65559",
            "97ef57a373464cdaaf3a2ba7962ec3f4",
            "63d304509713407ea44e848f78b6b404",
            "08274fd22ea24b0ab7b0eabea50c5817",
            "a65f26d0c92e45f7970a9465251f0f0f",
            "bcce1a28396a465db99e3281b495ee59",
            "59eb33891dd14357aeba1a1f381a2459",
            "42b7a45c750f40c1a36d3c842f7090c4"
          ]
        },
        "id": "dnGLVT7SPVKV",
        "outputId": "4f957e9b-9ad6-463d-900f-41c1eec99bb4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6755 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84aaea2896d04b669e38e75154a21a43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1689 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db79025b44a9439cb74bdf3f97ab5193"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 6755\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1689\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Purpose of Attention Mask:\n",
        "\n",
        "  It tells the model which tokens are real input tokens and which are padding tokens.\n",
        "  For real tokens, the mask value is 1; for padding tokens, it's 0.\n",
        "  This allows the model to ignore padding tokens and focus only on real input content.\n",
        "\n",
        "- Why Attention Mask is Needed:\n",
        "\n",
        "  In batch processing, sequences of different lengths are padded to the same length.\n",
        "  Attention Mask helps the model identify which parts are real data and which are padding.\n",
        "  This is crucial for correctly calculating attention and loss."
      ],
      "metadata": {
        "id": "6l4VnCD_ZCyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_datasets[\"train\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "711n1HqdPWvV",
        "outputId": "01a79450-30cc-4b91-999c-e920d17ceab9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 6755\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>PEFT: Parameter-Efficient Fine-Tuning"
      ],
      "metadata": {
        "id": "UWcSxWGNZgjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PromptEncoderConfig, PromptEncoderReparameterizationType, IA3Config, PeftModel, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training"
      ],
      "metadata": {
        "id": "z6SzngLeQZ5_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PromptEncoderConfig:\n",
        "\n",
        "  Used to configure settings for the prompt encoder.\n",
        "  Prompt encoding is a technique to adjust model behavior without changing the original model parameters.\n",
        "\n",
        "\n",
        "- PromptEncoderReparameterizationType:\n",
        "\n",
        "  Defines the reparameterization type for the prompt encoder.\n",
        "  Reparameterization can help optimize the training process.\n",
        "\n",
        "\n",
        "- IA3Config:\n",
        "\n",
        "  Configuration class for IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations).\n",
        "  IA3 is a parameter-efficient fine-tuning method.\n",
        "\n",
        "\n",
        "- PeftModel:\n",
        "\n",
        "  Base class for PEFT models, used to create various types of PEFT models.\n",
        "\n",
        "\n",
        "- get_peft_model:\n",
        "\n",
        "  A function to convert a regular Transformer model into a PEFT model.\n",
        "\n",
        "\n",
        "- LoraConfig:\n",
        "\n",
        "  Configuration class for LoRA (Low-Rank Adaptation).\n",
        "  LoRA is a popular parameter-efficient fine-tuning method.\n",
        "\n",
        "\n",
        "- TaskType:\n",
        "\n",
        "  Defines different task types, such as sequence-to-sequence, causal language modeling, etc.\n",
        "\n",
        "\n",
        "- prepare_model_for_kbit_training:\n",
        "\n",
        "  Function to prepare a model for k-bit training.\n",
        "  k-bit training is a quantization training technique that can reduce model memory usage.\n"
      ],
      "metadata": {
        "id": "Gj3O8neiaKAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>checking model structure"
      ],
      "metadata": {
        "id": "2SLkBTt0aRtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4MP38yMFQjl",
        "outputId": "98e48bc8-d164-46a7-c893-7fc0d7576b5b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(102400, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Method 1: IA3, Infused Adapter by inhibiting and Amplifying Inner Activations"
      ],
      "metadata": {
        "id": "DUQN_8tmFRTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Characteristics of the IA3 method:\n",
        "\n",
        "  It fine-tunes the model by injecting adapters into the model's internal activations.\n",
        "  \"Inhibiting and Amplifying\" refers to its ability to suppress or enhance certain internal features of the model.\n",
        "  This method can effectively adapt to new tasks while adjusting very few parameters.\n",
        "\n",
        "- Advantages of using IA3:\n",
        "\n",
        "  High parameter efficiency: Only a small number of additional parameters need to be trained.\n",
        "  Strong adaptability: Can effectively adjust the model to suit specific tasks.\n",
        "  Computational efficiency: Both training and inference are faster compared to full-parameter fine-tuning.\n",
        "\n",
        "- This configuration is particularly suitable for:\n",
        "\n",
        "  Situations requiring quick adaptation to new tasks.</br>\n",
        "  Environments with limited computational resources.</br>\n",
        "  Scenarios where maintaining most of the original model's knowledge is necessary."
      ],
      "metadata": {
        "id": "r-aTfcJccJ7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = IA3Config(task_type=TaskType.CAUSAL_LM)"
      ],
      "metadata": {
        "id": "cKOF_2feFQmc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ia3_model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "yx8OTwjBFQp0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ia3_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItiO9_mbHt8T",
        "outputId": "b0a4f437-dcd3-459d-e304-482e5f0272a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): IA3Model(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(102400, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-29): 30 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (k_proj): ia3.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 4096x1 (cuda:0)])\n",
              "              )\n",
              "              (v_proj): ia3.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 4096x1 (cuda:0)])\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): ia3.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x11008 (cuda:0)])\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ia3_model = ia3_model.to(torch.bfloat16)"
      ],
      "metadata": {
        "id": "Dhm4O3sPiP-N"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ia3_model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPJ5SyssHw1T",
        "outputId": "493b5780-2b09-42b8-9ba7-fad7dccb53db"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 576,000 || all params: 6,910,941,696 || trainable%: 0.0083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(ia3_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZe2o8FBduNw",
        "outputId": "8b1488c9-287b-4d8c-a9ba-790a4218fdb9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'peft.peft_model.PeftModelForCausalLM'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "RY3Zw-2ndNHg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir = \"/content/sample_data/\",\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 8,\n",
        "    logging_steps = 10,\n",
        "    num_train_epochs = 1,\n",
        "    bf16=True,\n",
        "    bf16_full_eval=True,\n",
        ")"
      ],
      "metadata": {
        "id": "bqlSav4tdNnZ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notification: New version of transformers will face ValueError as below:\n",
        "\n",
        "ValueError: You cannot perform fine-tuning on purely quantized models.\n",
        "\n",
        "Solution: !pip install transformers==4.33.0"
      ],
      "metadata": {
        "id": "Mn5Cm9KihqGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=ia3_model,\n",
        "    args=args,\n",
        "    train_dataset=processed_datasets[\"train\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, return_tensors='pt', mlm=False)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mm1Hd77dNq_",
        "outputId": "af85112b-6dd3-438a-8233-c6269e812014"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L4 GPU: training time is around 1.5 hours"
      ],
      "metadata": {
        "id": "UIUeO47-jPCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "FPrDpG-MdRLB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Method 2: QLoRA, Low-Rank Adaptation"
      ],
      "metadata": {
        "id": "HoJeOatwFMII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRA technology: this method combines the advantages of quantization (reduced memory usage) with the efficiency of LoRA (parameter-efficient fine-tuning), enabling effective fine-tuning of large language models with limited resources.\n",
        "\n",
        "## Explanation of LoRA vs IA3\n",
        "\n",
        "  Following code uses the LoRA (Low-Rank Adaptation) technique, which differs from the previously discussed IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations).\n",
        "\n",
        "- LoRA vs IA3:\n",
        "\n",
        "  LoRA: Adapts pre-trained models by adding low-rank matrices.\n",
        "\n",
        "  IA3: Adjusts models by inhibiting and amplifying internal activations.\n",
        "\n",
        "- Main Advantages of LoRA:\n",
        "\n",
        "  High parameter efficiency: Adds only a small number of trainable parameters.\n",
        "\n",
        "  Computational efficiency: Fast training and inference.\n",
        "\n",
        "  Flexibility: Easy to switch or combine different LoRA adapters.\n",
        "\n",
        "  In summary, this code sets up a PEFT configuration using LoRA technology for a causal language modeling task, primarily adjusting the projection matrices in the attention mechanism. Compared to IA3, LoRA offers a different approach to parameter-efficient fine-tuning, potentially performing better or being easier to adjust for certain tasks."
      ],
      "metadata": {
        "id": "nMuiEQkBkKq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LoRA configuration for a causal language modeling task\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=2,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        ")"
      ],
      "metadata": {
        "id": "ozaOWPSQSRKj"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.enable_input_require_grads()"
      ],
      "metadata": {
        "id": "TraBpz31SQ_b"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PeftModel = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "b06mQac8bll1"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PeftModel.enable_input_require_grads()"
      ],
      "metadata": {
        "id": "9JTn0QGhbn6w"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PeftModel.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiZrz9sVl6jk",
        "outputId": "f22c41c2-507a-4d95-d4ba-76f0f53cb432"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,966,080 || all params: 6,912,907,776 || trainable%: 0.0284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir = \"/content/sample_data/\",\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 8,\n",
        "    logging_steps = 10,\n",
        "    num_train_epochs = 1,\n",
        ")"
      ],
      "metadata": {
        "id": "PNuI0Ff4Sl39"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = PeftModel,\n",
        "    args = args,\n",
        "    train_dataset = processed_datasets[\"train\"],\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, return_tensors = 'pt', mlm=False)\n",
        ")"
      ],
      "metadata": {
        "id": "sOTRQPMaS1Ir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab03f1c9-c286-40d0-c487-3d582a2b7bba"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L4 GPU: training time is around 1:44 hours"
      ],
      "metadata": {
        "id": "GiusjHipmQrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "V7dVTJByS050"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Method 3: p-Tuning"
      ],
      "metadata": {
        "id": "pLwNHIqDlybK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "P-tuning is a parameter-efficient fine-tuning technique specifically designed for large language models. Here are the main features and working principles of P-tuning:\n",
        "\n",
        "- Basic Concept:\n",
        "  P-tuning focuses on optimizing the model's prompts.\n",
        "  It replaces manually designed discrete prompts with learned continuous virtual tokens.\n",
        "\n",
        "- Working Mechanism:\n",
        "  Adds trainable embedding vectors at the beginning of the input sequence.\n",
        "  These embedding vectors are called \"virtual tokens\" and their values are optimized during training.\n",
        "  Virtual tokens act as a soft prompt, guiding the model to generate task-specific outputs.\n",
        "\n",
        "- Advantages:\n",
        "  High parameter efficiency: Only a small number of parameters need to be trained.\n",
        "  Flexibility: Can adapt to different tasks and domains.\n",
        "  Performance: Can achieve performance comparable to full model fine-tuning on certain tasks.\n",
        "\n",
        "- Applications:\n",
        "  Particularly suitable for few-shot and zero-shot learning scenarios.\n",
        "  Performs well in various natural language processing tasks such as text classification, named entity recognition, etc.\n",
        "\n",
        "- Comparison with Other Methods:\n",
        "  Unlike traditional fine-tuning, P-tuning only modifies the input layer without changing other parts of the model.\n",
        "  Compared to LoRA, P-tuning focuses on optimizing the input layer rather than the internal weights of the model.\n",
        "\n",
        "- Implementation:\n",
        "  Usually requires modification of the model's input processing part.\n",
        "  In Hugging Face's Transformers library, it can be implemented by customizing the tokenizer and model architecture.\n",
        "\n",
        "  P-tuning represents a new approach of adapting to new tasks by optimizing the input rather than directly modifying model parameters. This method achieves efficient task adaptation while keeping most of the model parameters unchanged.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bG46aWVToPbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = PromptEncoderConfig(task_type = TaskType.CAUSAL_LM,\n",
        "                num_virtual_tokens = 10)\n",
        "\n",
        "pt_model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "fCG6SqW5lypI"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pt_model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1me28wLlzJO",
        "outputId": "59239be0-13b9-466a-e256-36ffe36d79f8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 50,384,896 || all params: 6,963,292,672 || trainable%: 0.7236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pt_model"
      ],
      "metadata": {
        "id": "dblreCUaaAX_",
        "outputId": "5f389699-ad4a-44ec-89d4-25834d825615",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LlamaForCausalLM(\n",
              "    (model): LlamaModel(\n",
              "      (embed_tokens): Embedding(102400, 4096)\n",
              "      (layers): ModuleList(\n",
              "        (0-29): 30 x LlamaDecoderLayer(\n",
              "          (self_attn): LlamaAttention(\n",
              "            (q_proj): lora.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=2, out_features=4096, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (k_proj): lora.Linear4bit(\n",
              "              (base_layer): ia3.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 4096x1 (cuda:0)])\n",
              "              )\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=2, out_features=4096, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (v_proj): lora.Linear4bit(\n",
              "              (base_layer): ia3.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 4096x1 (cuda:0)])\n",
              "              )\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=2, out_features=4096, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (o_proj): lora.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=2, out_features=4096, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (rotary_emb): LlamaRotaryEmbedding()\n",
              "          )\n",
              "          (mlp): LlamaMLP(\n",
              "            (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "            (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "            (down_proj): ia3.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "              (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x11008 (cuda:0)])\n",
              "            )\n",
              "            (act_fn): SiLUActivation()\n",
              "          )\n",
              "          (input_layernorm): LlamaRMSNorm()\n",
              "          (post_attention_layernorm): LlamaRMSNorm()\n",
              "        )\n",
              "      )\n",
              "      (norm): LlamaRMSNorm()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
              "  )\n",
              "  (prompt_encoder): ModuleDict(\n",
              "    (default): PromptEncoder(\n",
              "      (embedding): Embedding(10, 4096)\n",
              "      (mlp_head): Sequential(\n",
              "        (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "        (3): ReLU()\n",
              "        (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (word_embeddings): Embedding(102400, 4096)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir = \"/content/sample_data/\",\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 8,\n",
        "    logging_steps = 10,\n",
        "    num_train_epochs = 1,\n",
        ")"
      ],
      "metadata": {
        "id": "hwy1lf53nS0G"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = pt_model,\n",
        "    args = args,\n",
        "    train_dataset = processed_datasets[\"train\"],\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, return_tensors = 'pt', mlm=False)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEM5WbVdowrk",
        "outputId": "cf1c2292-7a85-4ff3-e71e-598da197336c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:447: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L4 GPU: training time is around 1:43 hours"
      ],
      "metadata": {
        "id": "DRLMoT4BpI2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "GyHDuf3gozdw"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "93MnbCStpM9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}